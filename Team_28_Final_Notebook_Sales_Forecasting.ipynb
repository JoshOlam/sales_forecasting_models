{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-11T09:24:53.643384Z",
     "start_time": "2021-06-11T09:24:53.622385Z"
    }
   },
   "source": [
    "# Sales Forecasting\n",
    "\n",
    "© Explore Data Science Academy\n",
    "\n",
    "---\n",
    "<a id=\"top\"></a>\n",
    "## EDSA Internship\n",
    "### Team 28\n",
    "#### Project Title: Develop an Accurate Forecasting Approach to Predict Future Sales Per Item Per Store\n",
    "\n",
    "#### Team Members\n",
    "- Joshua Olalemi\n",
    "- Koketso Tjale\n",
    "- Shadrack Darku\n",
    "- Ifeanyi Okonkwo\n",
    "- Thapelo Mofokeng\n",
    "---\n",
    "<img src=\"resources/supermarket.jpg\">\n",
    "\n",
    "--- "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Useful  Links\n",
    "* [Dipanshu Rana](https://github.com/ml-projects-rana/M5-Forecasting---Accuracy)\n",
    "\n",
    "* [Laura Fink](https://www.kaggle.com/code/allunia/m5-sales-uncertainty-prediction)\n",
    "\n",
    "* [MARISAKA MOZZ](https://www.kaggle.com/code/marisakamozz/m5-prophet/notebook)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents <a id=\"cont\"></a>\n",
    "\n",
    "\n",
    "\n",
    "### 1. [Importing Packages](#one) <br>\n",
    "\n",
    "\n",
    "### 2. [Loading Data](#two) <br>\n",
    "\n",
    "   \n",
    "   2.1 [Helper Functions](#2.1) <br>\n",
    "\n",
    "\n",
    "### 3. [Exploratory Data Analysis (EDA)](#three) <br>\n",
    "\n",
    "   \n",
    "   3.1 [Non-Graphical Analysis](#3.1) <br>\n",
    "   3.2 [Graphical Analysis](#3.2) <br>\n",
    "\n",
    "\n",
    "### 4. [Features Engineering](#four) <br>\n",
    "\n",
    "   \n",
    "   4.1 [Melting](#4.1) <br>\n",
    "   4.2 [Merging](#4.2) <br>\n",
    "   4.3 [Lag Features](#4.3) <br>\n",
    "   4.4 [Rolling_Mean Features](#4.4) <br>\n",
    "   4.5 [Dropping Redundant Features](#4.5) <br>\n",
    "   4.6 [Label Encoding](#4.6) <br>\n",
    "   4.7 [Saving Engineered Features](#4.7) <br>\n",
    "   4.8 [Releasing Memory](#4.8) <br>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### 5. [Modeling](#five) <br>\n",
    "\n",
    "   \n",
    "   5.1 [FacebookProphet](#5.1) <br>\n",
    "   5.2 [LightGBM](#5.2) <br>\n",
    "   5.4 [TensorFlow LSTM](#5.3) <br>\n",
    "\n",
    "\n",
    "### 6. [Model Performance](#six) <br>\n",
    "\n",
    "\n",
    "### 7. [Model Explanations](#seven) <br>\n",
    "\n",
    "\n",
    "### 8. [References](#eight) <br>\n",
    "\n",
    "\n",
    "### 8. [Acknowledgement ](#nine) <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <a id=\"one\"></a>\n",
    "## 1. Importing Packages\n",
    "<a href=#cont>Back to Table of Contents</a>\n",
    "\n",
    "---\n",
    "    \n",
    "| ⚡ Description: Importing Packages ⚡ |\n",
    "| :--------------------------- |\n",
    "| In this section, we imported and briefly discussed the libraries that were used throughout our analysis and modelling. |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-23T10:30:53.800892Z",
     "start_time": "2021-06-23T10:30:50.215449Z"
    }
   },
   "outputs": [],
   "source": [
    "#ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "#For collecting 'garbage' and releasing memory\n",
    "import gc\n",
    "from downcast import reduce\n",
    "\n",
    "#for converting lists to table format\n",
    "import tabletext\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "#for plots\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "import plotly.express as px\n",
    "import plotly.offline as py\n",
    "py.init_notebook_mode()\n",
    "\n",
    "#data preprocessing\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, LabelEncoder\n",
    "from sklearn.neighbors import KDTree\n",
    "\n",
    "#for saving file\n",
    "import pickle\n",
    "\n",
    "#cell timing\n",
    "import time\n",
    "\n",
    "#for progress bar visualization\n",
    "from tqdm import tqdm\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "#Modelling\n",
    "from prophet import *\n",
    "from fbprophet import Prophet\n",
    "from fbprophet.plot import plot_plotly\n",
    "\n",
    "from multiprocessing import Pool\n",
    "\n",
    "\n",
    "from lightgbm import LGBMRegressor\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import time\n",
    "import math\n",
    "import datetime\n",
    "\n",
    "\n",
    "# Import widgets\n",
    "from ipywidgets import widgets, interactive, interact\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "\n",
    "from math import log, floor\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set_style('whitegrid')\n",
    "\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import plotly.figure_factory as ff\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "import pywt\n",
    "from statsmodels.robust import mad\n",
    "\n",
    "import scipy\n",
    "import statsmodels\n",
    "from scipy import signal\n",
    "\n",
    "from statsmodels.tsa.arima_model import ARIMA\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "from statsmodels.tsa.api import ExponentialSmoothing, SimpleExpSmoothing, Holt\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "\n",
    "import itertools\n",
    "from itertools import cycle\n",
    "plt.style.use('seaborn')\n",
    "color_cycle = cycle(plt.rcParams['axes.prop_cycle'].by_key()['color'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"two\"></a>\n",
    "## 2. Loading the Data\n",
    "<a class=\"anchor\" id=\"1.1\"></a>\n",
    "<a href=#cont>Back to Table of Contents</a>\n",
    "\n",
    "---\n",
    "    \n",
    "| ⚡ Description: Loading the data ⚡ |\n",
    "| :--------------------------- |\n",
    "| In this section we loaded the given data into a `DataFrame`. |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"2.1\"></a>\n",
    "### 2.1 Helper Functions\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to the hugeness of the datasets used, we had to optimize the memory usage by creating some functions to convert the columns to efficient datatypes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-28T08:49:35.311495Z",
     "start_time": "2021-06-28T08:49:35.295494Z"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "def load_csv_optimized(path: str) -> pd.DataFrame:\n",
    "    \"\"\"Load the CSV and cast to more memory efficient dtypes.\n",
    "\n",
    "    Note: This is a minimal example, you can extend this function to handle additional things like columns you want excluded, etc.\n",
    "    \"\"\"\n",
    "    _df = pd.read_csv(\n",
    "        path,\n",
    "        parse_dates=True,\n",
    "        infer_datetime_format=True,\n",
    "        low_memory=False,\n",
    "    )\n",
    "    _df.convert_dtypes()\n",
    "\n",
    "    fcols = _df.select_dtypes(\"float\").columns\n",
    "    icols = _df.select_dtypes(\"integer\").columns\n",
    "    ccols = _df.select_dtypes(\"object\").columns\n",
    "\n",
    "    _df[fcols] = _df[fcols].apply(pd.to_numeric, downcast=\"float\")\n",
    "    _df[icols] = _df[icols].apply(pd.to_numeric, downcast=\"integer\")\n",
    "    _df[ccols] = _df[ccols].astype(\"category\")\n",
    "\n",
    "    print(\n",
    "        f\"Successfully loaded {path} using {_df.memory_usage(index=True,deep=True).sum()/1024/1024:.2f}MB RAM\"\n",
    "    )\n",
    "\n",
    "    return _df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "def reduce_mem_usage(df, verbose=True):\n",
    "    \"\"\"\n",
    "    This function takes in a dataframe, changes its datatypes and returns it.\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    return to improvise on this function based on this concept\n",
    "    integers = {\n",
    "                'int8':'np.iinfo(np.int8)',\n",
    "                'int16': 'np.iinfo(np.int16)',\n",
    "                'int32':'np.iinfo(np.int32)',\n",
    "                'int64':'np.iinfo(np.int64)'\n",
    "                }\n",
    "    \n",
    "    floats = {\n",
    "            'float16':'np.ninfo(np.int16)',\n",
    "            'float32':'np.finfo(np.float32)',\n",
    "            'float64':'np.finfo(np.float64)'\n",
    "             }\n",
    "    \"\"\"\n",
    "    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "    start_mem = df.memory_usage().sum() / 1024**2    \n",
    "    for col in df.columns: #get the columns in df\n",
    "        col_type = df[col].dtypes #get the datatypes of the columns\n",
    "        if col_type in numerics: #numerics\n",
    "            min_value = df[col].min()\n",
    "            max_value = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if min_value > np.iinfo(np.int8).min and max_value < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif min_value > np.iinfo(np.int16).min and max_value < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif min_value > np.iinfo(np.int32).min and max_value < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif min_value > np.iinfo(np.int64).min and max_value < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            else:\n",
    "                if min_value > np.finfo(np.float16).min and max_value < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif min_value > np.finfo(np.float32).min and max_value < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)    \n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#Load the needed datasets using the created function above\n",
    "df_calendar = load_csv_optimized(\"~/sales_data/raw_data/calendar.csv\")\n",
    "df_sell_prices = load_csv_optimized(\"~/sales_data/raw_data/sell_prices.csv\")\n",
    "df_train = load_csv_optimized(\"~/sales_data/raw_data/sales_train_evaluation.csv\")\n",
    "#df_test = load_csv_optimized(\"~/sales_data/raw_data/sales_train_validation.csv\")\n",
    "#df_submission = load_csv_optimized(\"~/sales_data/raw_data/sample_submission.csv\") # consider loading this later, you just need to know the structure if you want to submit to kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#Optimize the datatypes of the datasets\n",
    "df_calendar = reduce_mem_usage(df_calendar)\n",
    "df_sell_prices = reduce_mem_usage(df_sell_prices)\n",
    "df_train = reduce_mem_usage(df_train)\n",
    "#df_test = reduce_mem_usage(df_test)\n",
    "#df_submission = reduce_mem_usage(df_submission)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"three\"></a>\n",
    "## 3. Exploratory Data Analysis (EDA)\n",
    "<a class=\"anchor\" id=\"1.1\"></a>\n",
    "<a href=#cont>Back to Table of Contents</a>\n",
    "\n",
    "---\n",
    "    \n",
    "| ⚡ Description: Exploratory data analysis ⚡ |\n",
    "| :--------------------------- |\n",
    "| In this section, we explored and performed an in-depth analysis of all the variables in the DataFrame. |\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Exploratory Data Analysis (EDA) we conducted helped us to understand the data we were working with without making any assumptions. This formed a vital component before we continued with the modelling phase, as it provided context and guidance on the course of action to take when developing the appropriate model.\n",
    "\n",
    "This was done in 2-fold, viz;\n",
    "- Non-Graphical Analysis\n",
    "- Graphical Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"3.1\"></a>\n",
    "### 3.1 Non-Graphical Analysis\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `df_calendar`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "df_calendar.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "df_calendar.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "df_calendar.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `df_sell_price`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sell_prices.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sell_prices.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sell_prices.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `df_train`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"3.2\"></a>\n",
    "### 3.2 Graphical Analysis\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def patch1(bar,ax):\n",
    "    for p in bar.patches:\n",
    "        width=p.get_width()\n",
    "        height=p.get_height()\n",
    "        x,y=p.get_xy()\n",
    "        ax.annotate('{}%'.format(height),(x+width/2,y+height*1.02),ha='center',fontsize=14, fontweight='bold')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def patch2(a,ax):\n",
    "    for p in a.patches:\n",
    "        width=p.get_width()\n",
    "        height=p.get_height()\n",
    "        x,y=p.get_xy()\n",
    "        ax.annotate('{:.2f}'.format(height),(x+width/2,y+height*1.02),ha='center',fontsize=13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "def days_with_event():\n",
    "    l=[]\n",
    "    l=np.unique(df_calendar[df_calendar['event_type_1'].notnull()]['d'].tolist())\n",
    "    df=pd.DataFrame([[\"event\",np.round((len(l)/df_calendar.shape[0]*100),2)],\n",
    "                     [\"no_event\",100-np.round((len(l)/df_calendar.shape[0]*100),2)]],\n",
    "                    columns=['Events','Percentage'])\n",
    "    fig,axes=plt.subplots(figsize=(10,8))\n",
    "    a = df.plot(kind='bar',y='Percentage',x='Events',width=0.4,ax=axes,color='#3b82f6',edgecolor='black')\n",
    "    patch1(a,axes)\n",
    "    plt.title('Days with Events/No Events',loc='center',fontsize=25,fontweight='bold')\n",
    "    axes.set_xlabel('Event',fontsize=18,labelpad=15)\n",
    "    axes.set_ylabel('No_of_Occurences',fontsize=18,labelpad=15)\n",
    "    plt.xticks(rotation='horizontal',fontsize=14)\n",
    "    plt.yticks(fontsize=14)\n",
    "    axes.legend(loc='upper left')\n",
    "    plt.savefig('../sales_data/notebook_resources/days_with_events')\n",
    "    plt.show()\n",
    "    \n",
    "days_with_event()\n",
    "del days_with_event"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "def temp_merge():\n",
    "    s = pd.melt(df_train,id_vars=['id','item_id','dept_id','cat_id','store_id','state_id'],\n",
    "              var_name='d',\n",
    "              value_name='demand')\n",
    "    s = pd.merge(s,df_calendar,\n",
    "               on='d',\n",
    "               how='left')\n",
    "    revenue = pd.merge(s,\n",
    "                       df_sell_prices,\n",
    "                       on=['item_id','store_id','wm_yr_wk'],\n",
    "                       how='left')\n",
    "    #Calculating total cost on that day (cost = no. of sales of item * sell price of item)\n",
    "    revenue['cost']=revenue.demand*revenue.sell_price \n",
    "    return revenue\n",
    "revenue = temp_merge()\n",
    "revenue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "def daily_sales():\n",
    "    d=revenue[['weekday','demand']]\n",
    "    d=d.groupby(by='weekday').sum('demand').reset_index()\n",
    "    d['percent']=np.round(d['demand']/d['demand'].sum()*100,2)\n",
    "    d=d.sort_values('percent')\n",
    "    fig,axes=plt.subplots(figsize=(12,8))\n",
    "    a=d.plot(kind='bar',x='weekday',y='percent',width=0.6,ax=axes,color='#3b82f6', edgecolor='black')\n",
    "    patch1(a,axes)\n",
    "    plt.title('Sales per Day (Ascending Sort)',loc='center',fontsize=25,pad='20',fontweight='bold')\n",
    "    axes.set_ylabel('Percentage',fontsize=18,labelpad=15)\n",
    "    axes.set_xlabel('Weekday',fontsize=18,labelpad=15)\n",
    "    plt.xticks(rotation='horizontal',fontsize=14)\n",
    "    plt.yticks(fontsize=14)\n",
    "    axes.get_legend().remove()\n",
    "    #plt.savefig(\"../sales_data/notebook_resources/daily_sales\")\n",
    "    plt.show()\n",
    "\n",
    "daily_sales()\n",
    "del revenue, daily_sales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def monthly_sales():\n",
    "    revenue = temp_merge()\n",
    "    d=revenue[['month','demand']]\n",
    "    d=d.groupby(by='month').sum('demand').reset_index()\n",
    "    d['percent']=np.round(d['demand']/d['demand'].sum()*100,2)\n",
    "    fig,axes=plt.subplots(figsize=(12,8))\n",
    "    a=d.plot(kind='bar',x='month',y='percent',width=0.6,ax=axes,color='#3b82f6', edgecolor='black')\n",
    "    patch1(a,axes)\n",
    "    plt.title('Sales per Month',loc='center',fontsize=25,pad='20',fontweight='bold')\n",
    "    axes.set_ylabel('Percentage',fontsize=18,labelpad=15)\n",
    "    axes.set_xlabel('Month',fontsize=18,labelpad=15)\n",
    "    months = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']\n",
    "    axes.set_xticklabels(months)\n",
    "    plt.xticks(rotation='horizontal',fontsize=14)\n",
    "    plt.yticks(fontsize=14)\n",
    "    axes.get_legend().remove()\n",
    "    plt.savefig(\"../sales_data/notebook_resources/monthly_sales\")\n",
    "    plt.show()\n",
    "    \n",
    "monthly_sales()\n",
    "del monthly_sales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def yearly_sales():\n",
    "    revenue = temp_merge()\n",
    "    d=revenue[['year','demand']]\n",
    "    d=d.groupby(by='year').sum('demand').reset_index()\n",
    "    d['percent']=np.round(d['demand']/d['demand'].sum()*100,2)\n",
    "    d=d.sort_values('year')\n",
    "    fig,axes=plt.subplots(figsize=(11,8))\n",
    "    a=d.plot(kind='bar',x='year',y='percent',width=0.6,ax=axes,color='#3b82f6', edgecolor='black')\n",
    "    patch1(a,axes)\n",
    "    plt.title('Sales per Year',loc='center',fontsize=25,pad='20',fontweight='bold')\n",
    "    axes.set_ylabel('Percentage',fontsize=18,labelpad=15)\n",
    "    axes.set_xlabel('Year',fontsize=18,labelpad=15)\n",
    "    plt.xticks(rotation='horizontal',fontsize=14)\n",
    "    plt.yticks(fontsize=14)\n",
    "    axes.get_legend().remove()\n",
    "    #plt.savefig(\"../sales_data/notebook_resources/yearly_sales\")\n",
    "    plt.show()\n",
    "    \n",
    "yearly_sales()\n",
    "del yearly_sales"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions on distribution of product & behavior across timeline\n",
    "\n",
    "Starting with the dataframe denoted by train_sales_df that has the item specific ('id'), locale specific ('store_id' , 'state_id') and sales days specific (d_1 to d_1913) information; let us first make necessary adjustments to separate the sales days so that analysis along item_id, store_id and dept_id can be more easily observed across sales days only. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_cols = [c for c in df_train.columns if 'd_' in c]\n",
    "df_train_ins = df_train.copy() #create an insurance copy of df_train\n",
    "df_train_ins['total_sales_all_days'] = df_train_ins[d_cols].sum(axis = 1)\n",
    "df_train_ins['avg_sales_all_days'] = df_train_ins[d_cols].mean(axis = 1)\n",
    "df_train_ins['median_sales_all_days'] = df_train_ins[d_cols].median(axis = 1)\n",
    "#train_sales_df.groupby(['id'])['total_sales_all_days'].sum().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Distribution of product_ids across categories?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_train_ins.groupby(['cat_id'])['id'].count().reset_index(name='total_entries')\n",
    "fig = px.pie(df, values='total_entries', names='cat_id', \n",
    "            color_discrete_sequence=px.colors.sequential.RdBu,\n",
    "            width = 750, height=450, title = 'Distribution of product_IDs across categories')\n",
    "fig.show()\n",
    "del df, d_cols"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Food items are the most sold out item that are followed by household items and then hobbies items."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_train_ins.groupby(['state_id'])['total_sales_all_days'].sum().reset_index()\n",
    "fig = px.pie(df, values='total_sales_all_days', names='state_id', \n",
    "            color_discrete_sequence=px.colors.sequential.Aggrnyl,\n",
    "            width = 750, height=450, title = 'Distribution of Total_Sales Across States')\n",
    "fig.show()\n",
    "del df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With respect to the total number of sales, it is evident once again that the number of items sold on total have the greatest contributing share in CA, followed by Texas and Wisconsin. Now is it the case with the total revenue generated as well? We'd find that out using the revenue dataframe "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df_train_ins.groupby(['cat_id'])['id'].count().reset_index(name='total_entries')\n",
    "df2 = df_train_ins.groupby(['cat_id', 'state_id'])['total_sales_all_days'].sum().reset_index()\n",
    "sns.set_style('whitegrid')\n",
    "sns.axes_style(style='ticks')\n",
    "fig, (ax1, ax2) = plt.subplots(1,2, figsize=(14,5))\n",
    "\n",
    "sns.barplot(x = 'cat_id', y='total_entries', data=df1, \n",
    "            palette='mako', ax=ax1)\n",
    "sns.barplot(x = 'cat_id', y='total_sales_all_days', hue='state_id', data=df2, \n",
    "            palette='magma', ax=ax2)\n",
    "\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()\n",
    "del df1, df2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above two plots indicate:\n",
    "- Most items sold belong to the FOODS category, followed by HOUSEHOLD and HOBBIES.\n",
    "- CA leads in the number of \"Total items\" sold in either category (FOODS, HOBBIES AND HOUSEHOLD) , while WISCONSIN lags behind TX in each category except FOODS. We would see whether the same difference translates in terms of revenue extracted off these states or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = df_train_ins.groupby(['cat_id', 'store_id'])['total_sales_all_days'].sum().reset_index()\n",
    "sns.set_style('whitegrid')\n",
    "sns.axes_style(style='ticks')\n",
    "fig, ax1 = plt.subplots(figsize=(14,5))\n",
    "\n",
    "sns.barplot(x = 'store_id', y='total_sales_all_days', hue='cat_id', data=df3, \n",
    "            palette='afmhot', ax=ax1)\n",
    "\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()\n",
    "del df3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_train_ins.groupby(['state_id', 'cat_id'])['id'].count().reset_index(name='num_sales_by_category')\n",
    "fig = px.bar(df, x=\"state_id\", y=\"num_sales_by_category\", \n",
    "             color=\"cat_id\", title=\"Distribution of Product_ids Count Across Categories & Each Locale\")\n",
    "fig.show()\n",
    "del df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A couple of points that could be drawn from this observation are:\n",
    "\n",
    "Most items have been sold in california\n",
    "Texas and Wisconsin stores have almost total sales i.e. during the same timeframe of 1913 days, same number of items had been sold in both Texas and Wisconsin. Would this observation hold true in the df_sell_prices (revenue dataset) ? Does the observation remain same across different store locations in both Texas and Wisonconsin?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Distribution of Items Across Department & Store_ids?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_train_ins.groupby(['dept_id', 'store_id', 'state_id', 'cat_id'])[df_train_ins.columns[6:]].sum().reset_index()\n",
    "df = df.sort_values('total_sales_all_days', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_dept = df['dept_id']\n",
    "x_store = df['store_id']\n",
    "\n",
    "def items_sold_per_days(x_spec,title_text, title):\n",
    "    \n",
    "    '''\n",
    "    returns plotly plots with drop down menus for specified parameter made in dataframe earlier\n",
    "    \n",
    "    inputs: x_spec (categorical feature on the x_axis), title_text(title on dropdown), \n",
    "            title (title of the plot)\n",
    "            \n",
    "    returns: plotly plots of categorical feature (x_axis) with dropdowns on specific \n",
    "    number of days        \n",
    "    '''\n",
    "    \n",
    "    cols = ['d_1', 'd_50', 'd_300', 'd_500', 'd_700', 'd_900', 'd_1100', 'd_1500', 'd_1700',\n",
    "        'total_sales_all_days', 'median_sales_all_days']\n",
    "\n",
    "    buttons1 = [dict(method = \"restyle\",\n",
    "                 args = [{'x': [x_spec, 'undefined'],\n",
    "                          'y': [df[cols[k]], 'undefined'],\n",
    "                          'visible':[True, False]}], \n",
    "                 label = cols[k])   for k in range(0, len(cols))]\n",
    "\n",
    "    fig = go.Figure()\n",
    "    fig.add_trace(go.Bar(x=x_spec, y = df['d_1'], name='Dept.Sales on day2',\n",
    "                     marker_color='Crimson'))\n",
    "\n",
    "    fig.update_layout(title_text= title_text,\n",
    "                  title_x= 0.4, width=750, height=450, \n",
    "                  margin=dict(t=100, b=20, l=0, r=0),\n",
    "                  autosize = False,\n",
    "                  updatemenus=[dict(active=0,\n",
    "                                    buttons=buttons1,\n",
    "                                    x=0.08,\n",
    "                                    y=1.13,\n",
    "                                    xanchor='left',\n",
    "                                    yanchor='top')\n",
    "                              ]); \n",
    "\n",
    "    fig.add_annotation( x=0.00,  y=1.13, showarrow=False, xref='paper', yref='paper', xanchor='left',\n",
    "                   text=\"With<br>\"+str(title));\n",
    "    fig.show()\n",
    "\n",
    "items_sold_per_days(df['store_id'], 'Distribution of Sales Made on Each Store', 'Stores')\n",
    "items_sold_per_days(df['state_id'], \"Distribution of Sales Made In Each State\", 'States')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data_analysis on these points helps make it clear that:\n",
    "- Regarding the distribution of sales across department ids, most sales have been made across \"FOODS_3\" category followed by most sales made across household_1 category\n",
    "- Stores location identity along with embedded state_ids  make it clear that the distribution of sales across Texas & Wisconsin stores are NOT the same , though the total number represented across categories (foods, houshold and hobbies) might have come the same.\n",
    "- The outperformers in each state of CA, TX and WI are the stores with ids CA_3, TX_2 and WI_3, respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Specific item outselling the most?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Since the df_train_sales contains the information about each specific item and the number of sales made, we can make a few observations regarding the most frequently purchased item too.\n",
    "\n",
    "- We could plot its behavior across the number of days to get a general gist of its sales pattern across given days. i.e d_1 to d_1913"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_ins.groupby(['id'])['total_sales_all_days'].sum().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the total number of sales have been grouped against specific item id i.e. 'id' parameter, it is clear that the item \"**FOODS_3_090_CA_3_validation**\" has clearly sold most units than any other item in the category followed by **\"FOODS_3_586_TX_2_validation\"**. i.e. the first item belongs to food_3 category and sold in the CA_3 store location. Similarly, the second one belongs to TX_2 store location (i.e. second store in Texas) also belonging to the same category of FOODS_3 which is also consistent with observations made before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_style('whitegrid')\n",
    "\n",
    "def plot_dailysales(spec_id):\n",
    "    \"\"\"\n",
    "    plots the behavior of dailysales of specific ids i.e. spec_id\n",
    "    \n",
    "    input: spec_id\n",
    "    returns : number of sales plotted across number of days \n",
    "    \"\"\"\n",
    "    d_cols = [c for c in df_train.columns if 'd_' in c]\n",
    "    df_train.loc[df_train['id'] == spec_id ].set_index('id')[d_cols]\\\n",
    "                .T\\\n",
    "                .plot(figsize = (13,2.5),\n",
    "                      title =  str(spec_id)+\"_item daily sales\", \n",
    "                      color = next(color_cycle) )\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "plot_dailysales('FOODS_3_090_CA_3_evaluation') \n",
    "plot_dailysales('FOODS_3_586_TX_2_evaluation')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Item ID outselling most in each category?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_agg = pd.DataFrame(df_train_ins.groupby(['id', 'cat_id', 'store_id'])['total_sales_all_days'].sum().sort_values(ascending=False))\n",
    "df_agg = df_agg.reset_index()\n",
    "df_agg.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the dataset has been arranged in descending order of total sales, it would be a lot easier to estimate the item_id ('id') outselling others in each category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The 3 item_ids outselling most in FOODS category are: {}\".format(list(df_agg.loc[df_agg['cat_id'] == 'FOODS']['id'][:3])))\n",
    "print(\"The 3 item_ids outselling most in HOUSEHOLDS category are: {}\".format(list(df_agg.loc[df_agg['cat_id'] == 'HOUSEHOLD']['id'][:3])))\n",
    "print(\"The 3 item_ids outselling most in HOBBIES category are: {}\".format(list(df_agg.loc[df_agg['cat_id'] == 'HOBBIES']['id'][:3])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_cols = [c for c in df_train.columns if 'd_' in c]\n",
    "df = pd.DataFrame({\"days\": list(df_train[df_train['id'] == 'FOODS_3_090_CA_3_evaluation'][d_cols].columns),\n",
    "                   \"sales_data\": list(df_train[df_train['id'] == 'FOODS_3_090_CA_3_evaluation'][d_cols].values.flatten())})\n",
    "\n",
    "del d_cols, df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fig, (ax1, ax2, ax3) = plt.subplots(3, 1, figsize=(14, 7))\n",
    "\n",
    "def plot_sample_sales(spec_id, sm_start, sm_end, samples_pick=50):\n",
    "    \"\"\"\n",
    "    plots sample sales data with selection point and ending point specified, along with\n",
    "    samples_pick point that specifies the samples picked after specified intervals\n",
    "    \n",
    "    input: spec_id (item_id or id), sm_start (sample_start), sm_end(sample_end),\n",
    "    samples_pick (samples picked after how many intervals)\n",
    "    \n",
    "    returns: outputs a graph of sample points plotted against daily sales data d_1 to d_1913\n",
    "    \"\"\"\n",
    "    d_cols = [c for c in df_train.columns if 'd_' in c]\n",
    "    fig, ax1 = plt.subplots(figsize=(13, 2.5))\n",
    "    \n",
    "    x1 = list(df_train[df_train['id'] == spec_id][d_cols]\\\n",
    "              .columns)[sm_start:sm_end]\n",
    "    y1 = list(df_train[df_train['id'] == spec_id][d_cols]\\\n",
    "              .values.flatten())[sm_start:sm_end]\n",
    "    \n",
    "    #this conversion for regplot only\n",
    "    x1 = [x.replace(\"d_\", \"\") for x in x1]\n",
    "    x1 = [int(x) for x in x1]\n",
    "    \n",
    "    #sns.lineplot(x=x1, y=y1, ax=ax1)\n",
    "    sns.regplot(x=x1, y=y1, order=10, ax=ax1)\n",
    "    ax1.set_ylabel(\"Number of Sales\")\n",
    "    ax1.set_xlabel(\"Days\")\n",
    "\n",
    "    ax1.set_xticks(x1[::samples_pick])\n",
    "    ax1.set_xticklabels(x1[::samples_pick], rotation=0)\n",
    "\n",
    "    fig.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_sample_sales('FOODS_3_090_CA_3_evaluation', 500, 1300)\n",
    "plot_sample_sales('FOODS_3_586_TX_2_evaluation', 500, 1300)\n",
    "plot_sample_sales('FOODS_3_090_CA_1_evaluation', 500, 1300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the above graphs, our regression model does a fairly good job of fitting the line on the sales trend observed between the days 500th to 900th, for the item ids 'FOODS_3_090_CA_3_validation' and 'FOODS_3_586_TX_2_validation'. The graph also points out to the similar trends of troughs and crests between the specific days pointing out towards the occurence of special occasions and events that are driving sales. (To do _explain on order of polynomial and take more cases of ids across foods, household categories across different stores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions on Sales Revenue?\n",
    "\n",
    "- First of all, we'd be interested in finding out the specific revenue with respect to each product? But since we have not been provided the dataset with ids (product_ids) in df_sell_prices(revenue dataframe) and there is a mismatch in dataset entries between the both datasets (df_train_sales with almost 30000 rows and df_sell_prices with almost 6M entries), therefore, atbest, a rough estimate could be made by merging both datasets.\n",
    "\n",
    "- Fortunately, Revenue Dataframe (df_sell_prices) has the categorical level data available, so we could make an estimation regarding the items sold in each category to see what is the specific price where most items are getting sold. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sell_prices.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# making a new column category out of the item_id \n",
    "df_sell_prices_ins = df_sell_prices.copy()\n",
    "df_sell_prices_ins['category'] = df_sell_prices_ins['item_id'].str.split(\"_\", expand=True)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Distribution of price among categories?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_style('whitegrid')\n",
    "#plt.figure(figsize=(15,5))\n",
    "\n",
    "def kde_plotting(df, category, bin_size, color, label):\n",
    "    \n",
    "    '''\n",
    "    plots the kde density plot of the continuous features of df specified\n",
    "    \n",
    "    inputs: df, category(whether, foods, household or hobbies), bin_size(bin size for histogram)\n",
    "            color (color of the plot), label (label to the plot)\n",
    "    returns: kde plots with logarithmic scale taken on x_axis\n",
    "            \n",
    "    '''\n",
    "    fig, ax1 = plt.subplots(figsize=(13, 2.5) )\n",
    "\n",
    "    sns.distplot(df[df['category'] == category]['sell_price'], \n",
    "               axlabel = label ,bins=bin_size, color = color, ax=ax1) \n",
    "\n",
    "    fig.tight_layout()\n",
    "    ax1.set_xscale('log')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "kde_plotting(df_sell_prices_ins, 'HOBBIES', 150, 'b', 'hobbies')   \n",
    "kde_plotting(df_sell_prices_ins, 'FOODS', 250, 'g', 'foods') \n",
    "kde_plotting(df_sell_prices_ins, 'HOUSEHOLD', 150, 'r', 'household') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Insights from these kde plots. \n",
    "\n",
    "- The probability distribution plot of the **household** items follows an almost normal distribution with a mean centered around a price of 5 Dollars and most items being sold within the 1 to 10 dollars range. This would indicate that most household items that are getting sold lie within the price bracket of 25 cents to 10 dollars\n",
    "- **Foods** items prices is a multimodal distribution indicating frequent variation in interest among food items purchased. The values occur both towards the relative higher price bracket as well as lower price bracket indicating that the degree of interest of consumers in food items is not only varied but that the Walmart stores have a catalogue of food items that are peaking consumer's interest across different categories. The price bracket in this case also happens to lie within 2 cents to 10 dollars with only very few items getting sold past that range\n",
    "- The probability distribution of **hobbies** related items prices indicates a mix of bimodal and multimodal distributions. This indicates that while a few items in specific category were sold more than others (first peak that lies in the area between 0.01 dollar to 1 dollar) there are also items towards a relative higher price bracket that have been also sold quite frequently enought to give it a multimodal distribution with small decreasing peaks indicative of decreasing interest in hobbies related items that are relatively expensive but sill significant enough to generate consumer interest. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Removing outliers to observe price distribution? \n",
    "#### **Quartile Method**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_outliers(df):\n",
    "    \n",
    "    '''\n",
    "    removes the outliers in continous features using quartile ranges\n",
    "    \n",
    "    inputs: df(df specified with continous features along side categorical features)\n",
    "    returns: df with removed outliers\n",
    "    '''\n",
    "    Q1=df.quantile(0.25)\n",
    "    Q3=df.quantile(0.75)\n",
    "    IQR=Q3-Q1\n",
    "    df_final=df[~((df<(Q1-1.5*IQR)) | (df>(Q3+1.5*IQR)))]\n",
    "    \n",
    "    return df_final\n",
    "\n",
    "df = df_sell_prices_ins[['category', 'sell_price']]\n",
    "df = remove_outliers(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_style('whitegrid')\n",
    "sns.axes_style(style='ticks')\n",
    "plt.figure(figsize=(13,3))\n",
    "sns.boxplot(y=df['category'], x=df['sell_price'])\n",
    "plt.show()\n",
    "del df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After removing most of the outliers, it is apparent that for \n",
    "- FOODS related items, 75 % of the items sold are those that are less than 4 dollars\n",
    "- HOBBIES related items, 75 % of the items sold were less than 6 dollars with an mean price centered around 3.25-3.5 dollars\n",
    "- HOUSEHOLD items, 75 % of the items sold were less than 6.5 dollars. \n",
    "\n",
    "It also represents that there are quite a few outliers in our price data. Since we had observed before using the kdeplots, that the distributions of the dataset were mostly skewed, we used the quartile method of removing the outliers. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distribution of sales on weekdays & special occasions?\n",
    "\n",
    "Our third dataset named, df_calendar, provides valuable information along the timeseries for the dataset of product_id. This dataset also contains information about special occasions, SNAP (Supplementary Nutrition Assistance Programme) in the USA and coupled with the product_id dataset i.e. df_train_sales would be helpful in observing sales along weekdays, specific dates and special occasions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_calendar_ins = df_calendar.copy()\n",
    "df_calendar_ins.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_calendar_ins.groupby(['event_name_1', 'event_type_1'])['wday'].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the distribution of data in 'event_name_1' and 'event_type_1' it is clear the data here relates to holidays which could reveal important trends when coupled with the information of sales made on the specific event."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems a few entries that have not been made to the 'event_name_1' attribute have been made available in a different category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating and including a new entry of days as well as merging the events_1 and event_2 into\n",
    "# a single new events_names and types category\n",
    "\n",
    "df_calendar_ins['days'] = [d.split('-')[2] for d in df_calendar_ins['date']]\n",
    "df_calendar_ins['events_names'] = pd.concat([df_calendar_ins['event_name_1'], df_calendar_ins['event_name_2']], \n",
    "                                        ignore_index=True)\n",
    "df_calendar_ins['events_types'] = pd.concat([df_calendar_ins['event_type_1'], df_calendar_ins['event_type_2']], \n",
    "                                        ignore_index=True)\n",
    "#calendar_df.drop(['event_name_1', 'event_name_2', 'event_type_1', 'event_type_2'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What are **SNAP_CA, SNAP_TX, SNAP_WI**?\n",
    "\n",
    "SNAP stands for \"Supplementary Nutrition Assistance Program\" that is a federal level program aimed at providing food essentials to low-income households. This program is geared towards providing the food essentials and within the current dataset, the catagories of household items and hobbies items do not fall within the requirements of this program.\n",
    "\n",
    "This program is only geared towards fighting the food hunger in america and only food related items can be purchased under this program."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_calendar_ins.groupby(['events_types'])['snap_CA'].value_counts().reset_index(name='counts')\n",
    "sns.set_style('whitegrid')\n",
    "sns.axes_style(style='ticks')\n",
    "plt.figure(figsize=(15,5))\n",
    "sns.barplot(x = 'events_types', y='counts', hue='snap_CA', data=df, palette='bwr')\n",
    "plt.show()\n",
    "del df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_calendar_ins.groupby(['events_names'])['snap_CA'].value_counts().reset_index(name='counts')\n",
    "sns.set_style('whitegrid')\n",
    "sns.axes_style(style='ticks')\n",
    "plt.figure(figsize=(13,3))\n",
    "sns.barplot(x = 'events_names', y='counts', hue='snap_CA', data=df, \n",
    "            order = df.sort_values(['counts'], ascending=False).events_names, \n",
    "            palette='OrRd')\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()\n",
    "del df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A look at both of these plots indicates the special occasions when the SNAP programme in CA were availed.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distribution of sales items vs sales revenue? \n",
    "Now that we have product_id df as well as revenue_df, we'll merge the dataset now to start exploring trends of item_specific_data and sale_price_specific data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#product id df (df_sales_train) vs revenue_df(df_sell_prices)\n",
    "df_sales_prices = df_train_ins.merge(df_sell_prices_ins, how='inner', left_index=True, right_index=True)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_sales_prices.groupby(['cat_id', 'state_id', 'store_id_x'])['sell_price'].sum().reset_index(name='total_revenue')\n",
    "df = df.sort_values(by='total_revenue', ascending=False)\n",
    "sns.set_style('whitegrid')\n",
    "sns.axes_style(style='ticks')\n",
    "fig, (ax1, ax2) = plt.subplots(1,2, figsize=(14,5))\n",
    "\n",
    "sns.barplot(x = 'state_id', y='total_revenue', data=df, \n",
    "            palette='coolwarm', ax=ax1)\n",
    "sns.barplot(x = 'cat_id', y='total_revenue', hue='state_id', data=df, \n",
    "            palette='plasma', ax=ax2)\n",
    "\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()\n",
    "del df, df_sales_prices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Despite the fact that sales prices data contains almost 6M entries, in the present case, we are only considering common entries between revenue df and product_ids df. A few significant insights have come forward, i.e. \n",
    "\n",
    "- Although we saw that California consistently was the one state where the unique product_id most sales were made, the most revenue collected came from the Wisconsin State stores.\n",
    "- Similarly, within the distribution of categories, WI and TX contrinute more sales revenue than the CA stores. \n",
    "- Wisconsin leads the revenue in FOODS and HOBBIES, while Texas leads the revenue in HOUSEHOLD.\n",
    "- CA tends to contribute the smallest revenue out of all three states, despite having the most sales of items in its stores locations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"four\"></a>\n",
    "## 4. Feature Engineering\n",
    "<a class=\"anchor\" id=\"1.1\"></a>\n",
    "<a href=#cont>Back to Table of Contents</a>\n",
    "\n",
    "---\n",
    "    \n",
    "| ⚡ Description: Feature engineering ⚡ |\n",
    "| :--------------------------- |\n",
    "| In this section, we cleaned the dataset, and engineered new features based on insights gained in the EDA section. |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "for col in df_calendar.columns:\n",
    "    if df_calendar[col].isnull().sum()>0:\n",
    "        df_calendar[col] = df_calendar[col].cat.add_categories('no_event').fillna('no_event')\n",
    "        print(df_calendar[col].dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#Adding feature 'is_weekend' which tells about that day is weekend or not\n",
    "f=lambda x: 1 if x<=2 else 0\n",
    "df_calendar['weekend']=df_calendar['wday'].map(f) \n",
    "df_calendar['weekend']=df_calendar['weekend'].astype(np.int8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "#Adding feature 'month_day' which tells day of the month\n",
    "dates = df_calendar[\"date\"].tolist()\n",
    "dates=[i.split(\"-\")[2] for i in dates]\n",
    "df_calendar[\"month_day\"]=dates\n",
    "df_calendar['month_day']=df_calendar['month_day'].astype(np.int8)\n",
    "\"\"\"\n",
    "for col in df_calendar.columns:\n",
    "    if df_calendar[col].dtype == 'int16':\n",
    "        df_calendar[col] = df_calendar[col].astype(np.int8)\n",
    "\"\"\"\n",
    "df_calendar.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#Adding feature 'month_week_number' which tells which week of the month\n",
    "df_calendar['month_week_number'] = (df_calendar['month_day']-1) // 7 + 1 \n",
    "df_calendar['month_week_number'] = df_calendar['month_week_number'].astype(np.int8)\n",
    "\n",
    "df_calendar['month_week_number'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#Adding feature 'events_per_day' which tells us number of events on particular day\n",
    "f=lambda x: 0 if x=='no_event' else 1\n",
    "df_calendar['events_per_day']=df_calendar['event_type_1'].map(f) \n",
    "index=df_calendar.index \n",
    "indices=index[df_calendar['event_type_2']!='no_event'].tolist()\n",
    "for i in indices:\n",
    "    df_calendar['events_per_day'][i]+=1\n",
    "df_calendar['events_per_day']=df_calendar['events_per_day'].astype(np.int8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"4.1\"></a>\n",
    "### 4.1 `Melting` of Dataframe\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# This is the cell that requires more memory to run\n",
    "\n",
    "# creating a single dataframe\n",
    "\n",
    "final_df = pd.melt(\n",
    "                    df_train,\n",
    "                    id_vars=[\"id\", \"item_id\", \"dept_id\", \"cat_id\", \"store_id\", \"state_id\"],\n",
    "                    var_name=\"d\",\n",
    "                    value_name=\"sales\",\n",
    "                    )\n",
    "del df_train\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "final_df#.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"4.2\"></a>\n",
    "### 4.2 `Merging` of Dataframe\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "final_df = pd.merge(\n",
    "                    final_df,\n",
    "                    df_calendar,\n",
    "                    on=\"d\",\n",
    "                    how=\"left\",\n",
    "                    )\n",
    "del df_calendar\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "final_df = pd.merge(\n",
    "                    final_df,\n",
    "                    df_sell_prices,\n",
    "                    on=[\"item_id\", \"store_id\", \"wm_yr_wk\"],\n",
    "                    how=\"left\",\n",
    "                    )\n",
    "del df_sell_prices\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "final_df = reduce_mem_usage(final_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "final_df['date'] = pd.to_datetime(final_df['date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "final_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "final_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "final_df = reduce(final_df)\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "final_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "final_df['sell_price'] = final_df['sell_price'].fillna(final_df.groupby('id')['sell_price'].transform('mean'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "final_df['sell_price'].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "final_df = reduce(final_df)\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"4.3\"></a>\n",
    "### 4.3 Engineering `lag` features\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "lags=[28,35,42,49,56,63,70]\n",
    "for i in tqdm(lags):\n",
    "    final_df['lag_'+str(i)] = final_df.groupby(['id'])['sales'].shift(i).astype('float16').fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "final_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"4.4\"></a>\n",
    "### 4.4 Engineering `rolling` mean features\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#Rolling-Mean\n",
    "window=[7,14,28,35,42]\n",
    "for i in tqdm(window):\n",
    "    final_df['rolling_median_'+str(i)]=final_df.groupby(['id'])['sales'].transform(lambda s: s.rolling(i,center=False).median()).astype('float16').fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "final_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"4.5\"></a>\n",
    "### 4.5 Dropping Redundant Features\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "final_df.drop(['weekday'], axis=1, inplace=True)\n",
    "final_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"4.6\"></a>\n",
    "### 4.6 Label Encoding\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "final_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "final_df.rename(columns = {'d': 'day'}, inplace = True)\n",
    "final_df['day'] = final_df['day'].str.split('_', expand=True).iloc[: , 1:].astype('int16')\n",
    "final_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "le=LabelEncoder()\n",
    "\n",
    "categories = [col for col in final_df.columns if str(final_df[col].dtypes) == 'category'][6:]\n",
    "for column_name in tqdm(categories):\n",
    "    final_df[column_name] = le.fit_transform(final_df[column_name]).astype('int16')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "final_df = reduce_mem_usage(final_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "final_df = reduce(final_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"4.7\"></a>\n",
    "### 4.7 Saving Engineered Features\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#final_df.to_pickle(\"../sales_data/final_df.pkl\")\n",
    "\n",
    "df_path = \"../sales_data/processed_data/final_df.pkl\"\n",
    "with open(df_path,'wb') as file:\n",
    "    pickle.dump(final_df, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"4.8\"></a>\n",
    "### 4.8 Releasing Memory\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del final_df, categories\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"five\"></a>\n",
    "## 5. Modelling\n",
    "<a class=\"anchor\" id=\"1.1\"></a>\n",
    "<a href=#cont>Back to Table of Contents</a>\n",
    "\n",
    "---\n",
    "    \n",
    "| ⚡ Description: Modelling ⚡ |\n",
    "| :--------------------------- |\n",
    "| In this section, we created 3 Machine Learning Models that are able to accurately forecast future sales. |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"5.1\"></a>\n",
    "### 5.1 FacebookProphet\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"resources/m5_levels_new.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explain the parameters of the model; yhat, yhat_lower, yhat_upper etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "calendar = load_csv_optimized('~/sales_data/raw_data/calendar.csv')\n",
    "train = load_csv_optimized(\"~/sales_data/raw_data/sales_train_evaluation.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmse(pred,value):\n",
    "    return np.sqrt(((pred-value)**2).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"5.1.1\"></a>\n",
    "### 5.1.1 Top_Level\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "def facebook_preprocess(df_calendar, df_train):\n",
    "    series_cols = df_train.columns[df_train.columns.str.contains(\"d_\")].values\n",
    "    timeseries = df_train[series_cols].sum().values\n",
    "    train_timeseries = timeseries[0:-28]\n",
    "    eval_timeseries = timeseries[-28::]\n",
    "    days = np.arange(1, len(series_cols)+1)\n",
    "    dates = calendar.iloc[0:len(timeseries)].date.values\n",
    "    df = pd.DataFrame(dates, columns=[\"ds\"])\n",
    "    df.loc[:, \"y\"] = timeseries\n",
    "    train_df = df.iloc[0:-28]\n",
    "    eval_df = df.iloc[-28::]\n",
    "    \n",
    "    return train_df, eval_df\n",
    "\n",
    "def facebook_predicting(train):\n",
    "    uncertainty_interval_width = 0.25\n",
    "    m = Prophet(interval_width=uncertainty_interval_width, daily_seasonality=True)\n",
    "    m.fit(train)\n",
    "    return m\n",
    "\n",
    "def facebook_future(m, period):\n",
    "    future = m.make_future_dataframe(periods=period)\n",
    "    forecast = m.predict(future)\n",
    "    col_int = ['ds', 'yhat', 'yhat_lower', 'yhat_upper']\n",
    "    forecast[col_int].head()\n",
    "    return forecast[col_int]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "train_, eval_ = facebook_preprocess(calendar, train)\n",
    "prophet_full = facebook_predicting(train_)\n",
    "pred = facebook_future(prophet_full, 28)\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "df_path = \"../sales_data/pickled_models/facebook_models/prophet_full_data.pkl\"\n",
    "with open(df_path,'wb') as file:\n",
    "    pickle.dump(prophet_full, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def facebook_plot(df_calendar, df_train):\n",
    "    series_cols = df_train.columns[df_train.columns.str.contains(\"d_\")].values\n",
    "    timeseries = df_train[series_cols].sum().values\n",
    "    train_timeseries = timeseries[0:-28]\n",
    "    eval_timeseries = timeseries[-28::]\n",
    "    days = np.arange(1, len(series_cols)+1)\n",
    "    plt.figure(figsize=(20,5))\n",
    "    plt.plot(days[0:-28], train_timeseries, label=\"Train\")\n",
    "    plt.plot(days[-28::], eval_timeseries, label=\"Validation\")\n",
    "    plt.title(\"Top-Level-1: Summed product sales of all stores and states\");\n",
    "    plt.legend()\n",
    "    plt.xlabel(\"Day\")\n",
    "    plt.ylabel(\"Unit sales\");\n",
    "    \n",
    "facebook_plot(calendar, train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "forecast = facebook_future(prophet_full, 28)\n",
    "plt.plot(forecast.iloc[-28::].yhat.values, 'o', label=\"predicted yhat\")\n",
    "plt.plot(eval_.y.values, 'o-', label=\"target\")\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"5.1.2\"></a>\n",
    "### 5.1.2 State_Level\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "for col in train['state_id'].unique():\n",
    "    #print(col)\n",
    "    if str(col) == \"CA\":\n",
    "        df_CA = train[train['state_id']==str(col)]\n",
    "        #print(df_CA)\n",
    "    elif str(col) == \"TX\":\n",
    "        df_TX = train[train['state_id']==str(col)]\n",
    "    else:\n",
    "        df_WI = train[train['state_id']==str(col)]\n",
    "    #print(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "train_ca, eval_ca = facebook_preprocess(calendar, df_CA)\n",
    "prophet_ca = facebook_predicting(train_ca)\n",
    "facebook_future(prophet_ca, 28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "df_path = \"../sales_data/pickled_models/facebook_models/prophet_state_ca.pkl\"\n",
    "with open(df_path,'wb') as file:\n",
    "    pickle.dump(prophet_ca, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "train_tx, eval_tx = facebook_preprocess(calendar, df_TX)\n",
    "prophet_tx = facebook_predicting(train_tx)\n",
    "facebook_future(prophet_tx, 28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "df_path = \"../sales_data/pickled_models/facebook_models/prophet_state_tx.pkl\"\n",
    "with open(df_path,'wb') as file:\n",
    "    pickle.dump(prophet_tx, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "train_wi, eval_wi = facebook_preprocess(calendar, df_WI)\n",
    "prophet_wi = facebook_predicting(train_wi)\n",
    "facebook_future(prophet_wi, 28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "df_path = \"../sales_data/pickled_models/facebook_models/prophet_state_wi.pkl\"\n",
    "with open(df_path,'wb') as file:\n",
    "    pickle.dump(prophet_wi, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "df_path = \"../sales_data/pickled_models/facebook_models/prophet_state_wi.pkl\"\n",
    "with open(df_path,'wb') as file:\n",
    "    pickle.dump(prophet_wi, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "del df_CA, df_TX, df_WI, train_ca, eval_ca, train_tx, eval_tx, train_wi, eval_wi, prophet_ca, prophet_tx, prophet_wi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"5.1.3\"></a>\n",
    "### 5.1.3 Store_Level\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "for col in train['store_id'].unique():\n",
    "    #print(col)\n",
    "    if str(col) == \"CA_1\":\n",
    "        df_CA_1 = train[train['store_id']==str(col)]\n",
    "    elif str(col) == \"CA_2\":\n",
    "        df_CA_2 = train[train['store_id']==str(col)]\n",
    "    elif str(col) == \"CA_3\":\n",
    "        df_CA_3 = train[train['store_id']==str(col)]\n",
    "    elif str(col) == \"CA_4\":\n",
    "        df_CA_4 = train[train['store_id']==str(col)]\n",
    "    elif str(col) == \"TX_1\":\n",
    "        df_TX_1 = train[train['store_id']==str(col)]\n",
    "    elif str(col) == \"TX_2\":\n",
    "        df_TX_2 = train[train['store_id']==str(col)]\n",
    "    elif str(col) == \"TX_3\":\n",
    "        df_TX_3 = train[train['store_id']==str(col)]\n",
    "    elif str(col) == \"WI_1\":\n",
    "        df_WI_1 = train[train['store_id']==str(col)]\n",
    "    elif str(col) == \"WI_2\":\n",
    "        df_WI_2 = train[train['store_id']==str(col)]\n",
    "    else:\n",
    "        df_WI_3 = train[train['store_id']==str(col)]\n",
    "    #print(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "train_ca_1, eval_ca_1 = facebook_preprocess(calendar, df_CA_1)\n",
    "prophet_ca_1 = facebook_predicting(train_ca_1)\n",
    "facebook_future(prophet_ca_1, 28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "df_path = \"../sales_data/pickled_models/facebook_models/prophet_store_ca_1.pkl\"\n",
    "with open(df_path,'wb') as file:\n",
    "    pickle.dump(prophet_ca_1, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "train_ca_2, eval_ca_2 = facebook_preprocess(calendar, df_CA_2)\n",
    "prophet_ca_2 = facebook_predicting(train_ca_2)\n",
    "facebook_future(prophet_ca_2, 28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "df_path = \"../sales_data/pickled_models/facebook_models/prophet_store_ca_2.pkl\"\n",
    "with open(df_path,'wb') as file:\n",
    "    pickle.dump(prophet_ca_2, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "train_ca_3, eval_ca_3 = facebook_preprocess(calendar, df_CA_3)\n",
    "prophet_ca_3 = facebook_predicting(train_ca_3)\n",
    "facebook_future(prophet_ca_3, 28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "df_path = \"../sales_data/pickled_models/facebook_models/prophet_store_ca_3.pkl\"\n",
    "with open(df_path,'wb') as file:\n",
    "    pickle.dump(prophet_ca_3, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "train_ca_4, eval_ca_4 = facebook_preprocess(calendar, df_CA_4)\n",
    "prophet_ca_4 = facebook_predicting(train_ca_4)\n",
    "facebook_future(prophet_ca_4, 28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "df_path = \"../sales_data/pickled_models/facebook_models/prophet_store_ca_4.pkl\"\n",
    "with open(df_path,'wb') as file:\n",
    "    pickle.dump(prophet_ca_4, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "train_tx_1, eval_tx_1 = facebook_preprocess(calendar, df_TX_1)\n",
    "prophet_tx_1 = facebook_predicting(train_tx_1)\n",
    "facebook_future(prophet_tx_1, 28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "df_path = \"../sales_data/pickled_models/facebook_models/prophet_store_tx_1.pkl\"\n",
    "with open(df_path,'wb') as file:\n",
    "    pickle.dump(prophet_tx_1, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "train_tx_2, eval_tx_2 = facebook_preprocess(calendar, df_TX_2)\n",
    "prophet_tx_2 = facebook_predicting(train_tx_2)\n",
    "facebook_future(prophet_tx_2, 28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "df_path = \"../sales_data/pickled_models/facebook_models/prophet_store_tx_2.pkl\"\n",
    "with open(df_path,'wb') as file:\n",
    "    pickle.dump(prophet_tx_2, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "train_tx_3, eval_tx_3 = facebook_preprocess(calendar, df_TX_3)\n",
    "prophet_tx_3 = facebook_predicting(train_tx_3)\n",
    "facebook_future(prophet_tx_3, 28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "df_path = \"../sales_data/pickled_models/facebook_models/prophet_store_tx_3.pkl\"\n",
    "with open(df_path,'wb') as file:\n",
    "    pickle.dump(prophet_tx_3, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "train_wi_1, eval_wi_1 = facebook_preprocess(calendar, df_WI_1)\n",
    "prophet_wi_1 = facebook_predicting(train_wi_1)\n",
    "facebook_future(prophet_wi_1, 28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "df_path = \"../sales_data/pickled_models/facebook_models/prophet_store_wi_1.pkl\"\n",
    "with open(df_path,'wb') as file:\n",
    "    pickle.dump(prophet_wi_1, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "train_wi_2, eval_wi_2 = facebook_preprocess(calendar, df_WI_2)\n",
    "prophet_wi_2 = facebook_predicting(train_wi_2)\n",
    "facebook_future(prophet_wi_2, 28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "df_path = \"../sales_data/pickled_models/facebook_models/prophet_store_wi_2.pkl\"\n",
    "with open(df_path,'wb') as file:\n",
    "    pickle.dump(prophet_ca_3, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "train_wi_3, eval_wi_3 = facebook_preprocess(calendar, df_WI_3)\n",
    "prophet_wi_3 = facebook_predicting(train_wi_3)\n",
    "facebook_future(prophet_wi_3, 28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "df_path = \"../sales_data/pickled_models/facebook_models/prophet_store_wi_3.pkl\"\n",
    "with open(df_path,'wb') as file:\n",
    "    pickle.dump(prophet_ca_3, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"5.1.4\"></a>\n",
    "### 5.1.4 Category_Level\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "df_cat_hobbies = train[train['cat_id'] == \"HOBBIES\"]\n",
    "train_hob, eval_hob = facebook_preprocess(calendar, df_cat_hobbies)\n",
    "prophet_cat_hob = facebook_predicting(train_hob)\n",
    "facebook_future(prophet_cat_hob, 28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "df_path = \"../sales_data/pickled_models/facebook_models/prophet_cat_hobbies.pkl\"\n",
    "with open(df_path,'wb') as file:\n",
    "    pickle.dump(prophet_cat_hob, file)\n",
    "    \n",
    "del df_cat_hobbies, train_hob, eval_hob, prophet_cat_hob, df_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "df_cat_household = train[train['cat_id'] == \"HOUSEHOLD\"]\n",
    "train_house, eval_house = facebook_preprocess(calendar, df_cat_household)\n",
    "prophet_cat_house = facebook_predicting(train_house)\n",
    "facebook_future(prophet_cat_house, 28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "df_path = \"../sales_data/pickled_models/facebook_models/prophet_cat_household.pkl\"\n",
    "with open(df_path,'wb') as file:\n",
    "    pickle.dump(prophet_cat_house, file)\n",
    "    \n",
    "del df_cat_household, train_house, eval_house, prophet_cat_house, df_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "df_cat_food = train[train['cat_id'] == \"FOODS\"]\n",
    "train_food, eval_food = facebook_preprocess(calendar, df_cat_food)\n",
    "prophet_cat_food = facebook_predicting(train_food)\n",
    "facebook_future(prophet_cat_food, 28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "df_path = \"../sales_data/pickled_models/facebook_models/prophet_cat_food.pkl\"\n",
    "with open(df_path,'wb') as file:\n",
    "    pickle.dump(prophet_cat_food, file)\n",
    "    \n",
    "del df_cat_food, train_food, eval_food, prophet_cat_food, df_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"5.1.5\"></a>\n",
    "### 5.1.5 Sub_Category_Level\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "df_cat_hobbies = train[train['dept_id'] == \"HOBBIES_1\"]\n",
    "train_hob, eval_hob = facebook_preprocess(calendar, df_cat_hobbies)\n",
    "prophet_cat_hob = facebook_predicting(train_hob)\n",
    "facebook_future(prophet_cat_hob, 28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "df_path = \"../sales_data/pickled_models/facebook_models/prophet_sub_cat_hobbies_1.pkl\"\n",
    "with open(df_path,'wb') as file:\n",
    "    pickle.dump(prophet_cat_hob, file)\n",
    "    \n",
    "del df_cat_hobbies, train_hob, eval_hob, prophet_cat_hob, df_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "df_cat_hobbies = train[train['dept_id'] == \"HOBBIES_2\"]\n",
    "train_hob, eval_hob = facebook_preprocess(calendar, df_cat_hobbies)\n",
    "prophet_cat_hob = facebook_predicting(train_hob)\n",
    "facebook_future(prophet_wi_2, 28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "df_path = \"../sales_data/pickled_models/facebook_models/prophet_sub_cat_hobbies_2.pkl\"\n",
    "with open(df_path,'wb') as file:\n",
    "    pickle.dump(prophet_cat_hob, file)\n",
    "    \n",
    "del df_cat_hobbies, train_hob, eval_hob, prophet_cat_hob, df_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "df_cat_household = train[train['dept_id'] == \"HOUSEHOLD_1\"]\n",
    "train_house, eval_house = facebook_preprocess(calendar, df_cat_household)\n",
    "prophet_cat_house = facebook_predicting(train_house)\n",
    "facebook_future(prophet_cat_house, 28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "df_path = \"../sales_data/pickled_models/facebook_models/prophet_sub_cat_house_1.pkl\"\n",
    "with open(df_path,'wb') as file:\n",
    "    pickle.dump(prophet_cat_house, file)\n",
    "    \n",
    "del df_cat_household, train_house, eval_house, prophet_cat_house, df_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "df_cat_household = train[train['dept_id'] == \"HOUSEHOLD_2\"]\n",
    "train_house, eval_house = facebook_preprocess(calendar, df_cat_household)\n",
    "prophet_cat_house = facebook_predicting(train_house)\n",
    "facebook_future(prophet_cat_house, 28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "df_path = \"../sales_data/pickled_models/facebook_models/prophet_sub_cat_house_2.pkl\"\n",
    "with open(df_path,'wb') as file:\n",
    "    pickle.dump(prophet_cat_house, file)\n",
    "    \n",
    "del prophet_cat_house, train_house, eval_house, df_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "df_cat_food = train[train['dept_id'] == \"FOODS_1\"]\n",
    "train_food, eval_food = facebook_preprocess(calendar, df_cat_food)\n",
    "prophet_cat_food = facebook_predicting(train_food)\n",
    "facebook_future(prophet_cat_food, 28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "df_path = \"../sales_data/pickled_models/facebook_models/prophet_sub_cat_food_1.pkl\"\n",
    "with open(df_path,'wb') as file:\n",
    "    pickle.dump(prophet_cat_food, file)\n",
    "    \n",
    "del df_cat_food, train_food, eval_food, df_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "df_cat_food = train[train['dept_id'] == \"FOODS_2\"]\n",
    "train_food, eval_food = facebook_preprocess(calendar, df_cat_food)\n",
    "prophet_cat_food = facebook_predicting(train_food)\n",
    "facebook_future(prophet_cat_food, 28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "df_path = \"../sales_data/pickled_models/facebook_models/prophet_sub_cat_food_2.pkl\"\n",
    "with open(df_path,'wb') as file:\n",
    "    pickle.dump(prophet_cat_food, file)\n",
    "    \n",
    "del df_cat_food, prophet_cat_food, train_food, eval_food, df_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "df_cat_food = train[train['dept_id'] == \"FOODS_3\"]\n",
    "train_food, eval_food = facebook_preprocess(calendar, df_cat_food)\n",
    "prophet_cat_food = facebook_predicting(train_food)\n",
    "facebook_future(prophet_cat_food, 28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "df_path = \"../sales_data/pickled_models/facebook_models/prophet_sub_cat_food_3.pkl\"\n",
    "with open(df_path,'wb') as file:\n",
    "    pickle.dump(prophet_cat_food, file)\n",
    "    \n",
    "del df_cat_food, train_food, eval_food, prophet_cat_food, df_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Putting all the bits together..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "AGGREGATION_LEVELS = [\n",
    "    [],\n",
    "    ['state_id'],\n",
    "    ['store_id'],\n",
    "    ['cat_id'],\n",
    "    ['dept_id'],\n",
    "    ['state_id', 'cat_id'],\n",
    "    ['state_id', 'dept_id'],\n",
    "    ['store_id', 'cat_id'],\n",
    "    ['store_id', 'dept_id'],\n",
    "    ['item_id'],\n",
    "    ['state_id', 'item_id'],\n",
    "    ['item_id', 'store_id']\n",
    "]\n",
    "INTERVALS = [0.99, 0.95, 0.75, 0.5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "def read_sales(filename):\n",
    "    calendar = pd.read_csv('../sales_data/raw_data/calendar.csv', parse_dates=['date'])\n",
    "    sales = pd.read_csv(filename)\n",
    "    agg_sales = []\n",
    "    for level in AGGREGATION_LEVELS:\n",
    "        if len(level) == 0:\n",
    "            agg = pd.DataFrame(sales.sum(numeric_only=True)).T\n",
    "            agg['id'] = 'Total_X'\n",
    "        elif len(level) == 1:\n",
    "            agg = sales.groupby(level).sum(numeric_only=True).reset_index()\n",
    "            agg['id'] = agg[level[0]] + '_X'\n",
    "            agg.drop(level, axis=1, inplace=True)\n",
    "        else:\n",
    "            agg = sales.groupby(level).sum(numeric_only=True).reset_index()\n",
    "            agg['id'] = agg[level[0]] + '_' + agg[level[1]]\n",
    "            agg.drop(level, axis=1, inplace=True)\n",
    "        agg_sales.append(agg)\n",
    "    sales = pd.concat(agg_sales)\n",
    "    sales.set_index('id', inplace=True)\n",
    "    sales.columns = calendar.date[:len(sales.columns)]\n",
    "    return sales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "def fit_model(params):\n",
    "    data, prefix, suffix = params\n",
    "    data = data.T.reset_index()\n",
    "    data.columns = ['ds', 'y']\n",
    "    quantiles = []\n",
    "    for interval in INTERVALS:\n",
    "        model = Prophet(interval_width=interval,\n",
    "                       daily_seasonality=True)\n",
    "        model.fit(data)\n",
    "        future = model.make_future_dataframe(periods=28)\n",
    "        forecast = model.predict(future)\n",
    "        quantile = forecast[['ds', 'yhat_lower', 'yhat_upper']].tail(28).copy()\n",
    "        lower = (1 - interval) / 2\n",
    "        upper = 1 - lower\n",
    "        quantile.columns = ['date', f'{prefix}_{lower:.3f}_{suffix}', f'{prefix}_{upper:.3f}_{suffix}']\n",
    "        quantile = quantile.set_index('date').T\n",
    "        quantile.index.name = 'id'\n",
    "        quantiles.append(quantile)\n",
    "    median = forecast[['ds', 'yhat']].tail(28).copy()\n",
    "    median.columns = ['date', f'{prefix}_0.500_{suffix}']\n",
    "    median = median.set_index('date').T\n",
    "    median.index.name = 'id'\n",
    "    quantiles.append(median)\n",
    "    quantiles = pd.concat(quantiles)\n",
    "    return quantiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "def forecast(sales, suffix='validation'):\n",
    "    sales_list = [(row, row.name, suffix) for _, row in sales.head(8).iterrows()]  # for kaggle env\n",
    "    # sales_list = [(row, row.name, suffix) for _, row in sales.iterrows()]\n",
    "    pool = Pool(4)\n",
    "    result = pool.map(fit_model, sales_list)\n",
    "    return pd.concat(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "sales_valid = read_sales('../sales_data/raw_data/sales_train_validation.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "sales_eval = read_sales('../sales_data/raw_data/sales_train_evaluation.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "def forecast_one(index=0):\n",
    "    data = sales_valid.iloc[index]\n",
    "    params = data, data.name, 'plot'\n",
    "    forecast_valid = fit_model(params)\n",
    "    data = sales_eval.iloc[index]\n",
    "    params = data, data.name, 'plot'\n",
    "    forecast_eval = fit_model(params)\n",
    "    data = pd.concat([forecast_valid, forecast_eval], axis=1)\n",
    "    data = pd.concat([sales_eval.iloc[index:index+1, -28:], data])\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "result = forecast_one()\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "result.T.plot(figsize=(16,9))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del sales_eval, sales_valid, result, AGGREGATION_LEVELS, INTERVALS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"5.2\"></a>\n",
    "### 5.2 LightGBModel\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import pickle\n",
    "df_path = \"../sales_data/processed_data/final_df.pkl\"\n",
    "with open(df_path,'rb') as file:\n",
    "    df = pickle.load(file)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "def rmse(pred,value):\n",
    "    return np.sqrt(((pred-value)**2).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "df=df[df['day']>1000]#.drop('date', axis=1, inplace=True) #to save memory, we use days greater 1,000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop('date', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "l=[]\n",
    "for i in range(1886,1914):\n",
    "  l.append(i)\n",
    "\n",
    "#https://www.kite.com/python/answers/how-to-select-rows-by-multiple-label-conditions-with-pandas-loc-in-python\n",
    "x_train=df.loc[df['day']<=1885]\n",
    "x_valid=df.loc[df['day'].isin(l)]\n",
    "x_test=df.loc[df['day']>=1914]\n",
    "\n",
    "y_train=x_train['sales']\n",
    "y_valid=x_valid['sales']\n",
    "y_test=x_test['sales']\n",
    "\n",
    "x_train=x_train.drop(['sales'],axis=1)\n",
    "x_valid=x_valid.drop(['sales'],axis=1)\n",
    "x_test=x_test.drop(['sales'],axis=1)\n",
    "\n",
    "print(\"x_train {}\".format(x_train.shape),\"  y_train {}\".format(y_train.shape))\n",
    "print(\"\\nx_valid {}\".format(x_valid.shape),\"  y_valid {}\".format(y_valid.shape))\n",
    "print(\"\\nx_test {}\".format(x_test.shape),\"  y_test {}\".format(y_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "for i in tqdm(range(15)):\n",
    "    learning_rate=np.round(np.random.uniform(0.001,0.05),4)\n",
    "    max_depth=np.random.randint(5,100)\n",
    "    num_leaves=np.random.randint(20,100)\n",
    "    lgb=LGBMRegressor(learning_rate=learning_rate,max_depth=max_depth,num_leaves=num_leaves,n_jobs=-1,n_estimators=100)\n",
    "    lgb.fit(x_train,y_train)\n",
    "    y_pred=lgb.predict(x_valid)\n",
    "    print(\"\\n\\nlearning_rate: {}\".format(learning_rate),\"  max_depth: {}\".format(max_depth),\n",
    "          \" num_leaves: {}\".format(num_leaves),\" Rmse: {}\".format(rmse(y_pred,y_valid)))\n",
    "    print(\"-\"*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "learning_rate=0.034 \n",
    "max_depth=66 \n",
    "num_leaves=224 \n",
    "lgb=LGBMRegressor(learning_rate=learning_rate,max_depth=max_depth,num_leaves=num_leaves,n_jobs=-1,n_estimators=100)\n",
    "lgb.fit(x_train,y_train)\n",
    "y_pred=lgb.predict(x_valid)\n",
    "print(\"learning_rate: {}\".format(learning_rate),\"  max_depth: {}\".format(max_depth),\"  num_leaves: {}\".format(num_leaves),\"  Rmse: {}\".format(rmse(y_pred,y_valid)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "features=x_train.columns\n",
    "imp=lgb.feature_importances_\n",
    "indices=(np.argsort(imp))[5:]\n",
    "plt.figure(figsize=(8,10))\n",
    "plt.title('Feature Importance',fontsize=14)\n",
    "plt.barh(range(len(indices)),imp[indices],color='r')\n",
    "plt.yticks(range(len(indices)),[features[i] for i in indices])\n",
    "plt.xlabel('Relative Importance',fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"5.3\"></a>\n",
    "### 5.3 TensorFlow LSTM\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We made use of the top level timeseries for an example.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "class MyLSTM(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dim, hidden_dim, batch_size, num_layers=1, output_dim=1):\n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.batch_size = batch_size\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        self.lstm = nn.LSTM(input_size=self.input_dim,\n",
    "                            hidden_size=self.hidden_dim,\n",
    "                            num_layers=self.num_layers,\n",
    "                            dropout = 0.25)\n",
    "        self.linear = nn.Linear(self.hidden_dim, output_dim)\n",
    "        \n",
    "    def init_hidden(self):\n",
    "        self.h_zero = torch.zeros(self.num_layers, self.batch_size, self.hidden_dim).to(device)\n",
    "        self.c_zero = torch.zeros(self.num_layers, self.batch_size, self.hidden_dim).to(device)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        lstm_output, (h_n, c_n) = self.lstm(x.view(len(x), self.batch_size, -1),\n",
    "                                           (self.h_zero, self.c_zero))\n",
    "        last_time_step = lstm_output.view(self.batch_size, len(x), self.hidden_dim)[-1]\n",
    "        pred = self.linear(last_time_step)\n",
    "        return pred\n",
    "    \n",
    "\n",
    "def train_model(model, data_dict, lr=1e-4, num_epochs=500):\n",
    "    \n",
    "    loss_fun = torch.nn.MSELoss(reduction=\"mean\")\n",
    "    optimiser = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    \n",
    "    train_losses = np.zeros(num_epochs)\n",
    "    phases = [\"train\", \"eval\"]\n",
    "    losses_dict = {\"train\": [], \"eval\": []}\n",
    "    predictions_dict = {\"train\": [], \"eval\": [] }\n",
    "    \n",
    "    for n in range(num_epochs):\n",
    "        \n",
    "        for phase in phases:\n",
    "            \n",
    "            x = data_dict[phase][\"input\"].to(device, dtype=torch.float)\n",
    "            y = data_dict[phase][\"target\"].to(device, dtype=torch.float)\n",
    "            \n",
    "            if phase == \"train\":\n",
    "                model.train()\n",
    "            else:\n",
    "                model.eval()\n",
    "        \n",
    "            optimiser.zero_grad()\n",
    "            \n",
    "            model.init_hidden()\n",
    "            y_pred = model(x)\n",
    "            \n",
    "            if n == (num_epochs-1):\n",
    "                predictions_dict[phase] = y_pred.float().cpu().detach().numpy()\n",
    "            \n",
    "            loss = loss_fun(y_pred.float(), y)\n",
    "            losses_dict[phase].append(loss.item())\n",
    "            \n",
    "            if n % 50 == 0:\n",
    "                print(\"{} loss: {}\".format(phase, loss.item()))\n",
    "            \n",
    "            if phase == 'train':\n",
    "                loss.backward()\n",
    "                optimiser.step()\n",
    "        \n",
    "    return losses_dict, predictions_dict\n",
    "\n",
    "def create_sequences(timeseries, seq_len):\n",
    "    inputs = []\n",
    "    targets = []\n",
    "    \n",
    "    max_steps = len(timeseries) - (seq_len+1)\n",
    "    \n",
    "    for t in range(max_steps):\n",
    "        x = timeseries[t:(t+seq_len)]\n",
    "        y = timeseries[t+seq_len]\n",
    "        inputs.append(x)\n",
    "        targets.append(y)\n",
    "    \n",
    "    return np.array(inputs), np.array(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "series_cols = train.columns[train.columns.str.contains(\"d_\")].values\n",
    "timeseries = train[series_cols].sum().values\n",
    "train_timeseries = timeseries[0:-28]\n",
    "eval_timeseries = timeseries[-28::]\n",
    "days = np.arange(1, len(series_cols)+1)\n",
    "dates = calendar.iloc[0:len(timeseries)].date.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To prepare the data for modelling, we will have to remove the timeseries and scale the values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "diff_series = np.diff(timeseries)\n",
    "train_size = np.int(0.7 * len(diff_series))\n",
    "train_diff_series = diff_series[0:train_size]\n",
    "eval_diff_series = diff_series[train_size::]\n",
    "scaler = MinMaxScaler(feature_range=(-1,1))\n",
    "scaled_train = scaler.fit_transform(train_diff_series.reshape(-1, 1))\n",
    "scaled_eval = scaler.transform(eval_diff_series.reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "fig, ax = plt.subplots(1,2,figsize=(20,5))\n",
    "ax[0].plot(scaled_train, '-o', c=\"b\")\n",
    "ax[1].plot(scaled_eval, '-o', c=\"g\")\n",
    "ax[0].set_title(\"Single preprocessed top timeseries in train\")\n",
    "ax[1].set_title(\"Single preprocessed top timeseries in eval\");\n",
    "ax[0].set_xlabel(\"Days in dataset\")\n",
    "ax[1].set_xlabel(\"Days in dataset\")\n",
    "ax[0].set_ylabel(\"$\\Delta y$ scaled\")\n",
    "ax[1].set_ylabel(\"$\\Delta y$ scaled\");\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's prepare the model for fitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "seq_len = 400\n",
    "input_dim = 1\n",
    "hidden_dim = 128\n",
    "num_epochs = 600\n",
    "lr=0.0005\n",
    "\n",
    "\n",
    "x_train, y_train = create_sequences(scaled_train, seq_len)\n",
    "x_eval, y_eval = create_sequences(scaled_eval, seq_len)\n",
    "\n",
    "x_train = torch.from_numpy(x_train).float()\n",
    "y_train = torch.from_numpy(y_train).float()\n",
    "x_eval = torch.from_numpy(x_eval).float()\n",
    "y_eval = torch.from_numpy(y_eval).float()\n",
    "\n",
    "data_dict = {\"train\": {\"input\": x_train, \"target\": y_train},\n",
    "             \"eval\": {\"input\": x_eval, \"target\": y_eval}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "model = MyLSTM(input_dim=input_dim,\n",
    "               hidden_dim=hidden_dim,\n",
    "               batch_size=seq_len)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "run_training = True\n",
    "if run_training:\n",
    "    losses_dict, predictions_dict = train_model(model, data_dict, num_epochs=num_epochs, lr=lr)\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "if run_training:\n",
    "    \n",
    "    fig, ax = plt.subplots(3,1,figsize=(20,20))\n",
    "    ax[0].plot(losses_dict[\"train\"], '.-', label=\"train\", c=\"red\")\n",
    "    ax[0].set_xlabel(\"Epochs\")\n",
    "    ax[0].set_ylabel(\"MSE\")\n",
    "    ax[0].plot(losses_dict[\"eval\"], '.-', label=\"eval\", c=\"blue\");\n",
    "    ax[0].legend();\n",
    "\n",
    "    ax[1].plot(predictions_dict[\"train\"], '-o', c=\"red\")\n",
    "    ax[1].plot(y_train, '-o', c=\"green\")\n",
    "    ax[1].set_title(\"Fitted and true values of y in train\");\n",
    "    ax[1].set_ylabel(\"Unit sales y\");\n",
    "    ax[1].set_xlabel(\"Number of days in train\");\n",
    "\n",
    "    ax[2].plot(predictions_dict[\"eval\"], '-o', c=\"red\")\n",
    "    ax[2].plot(y_eval, '-o', c=\"green\")\n",
    "    ax[2].set_title(\"Predicted and true values of y in eval\");\n",
    "    ax[2].set_xlabel(\"Number of days in eval\");\n",
    "    ax[2].set_ylabel(\"Unit sales y\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from statsmodels.graphics.tsaplots import plot_acf\n",
    "\n",
    "if run_training:\n",
    "    \n",
    "    train_residuals = y_train-predictions_dict[\"train\"]\n",
    "    eval_residuals = y_eval-predictions_dict[\"eval\"]\n",
    "    \n",
    "    fig, ax = plt.subplots(2,2,figsize=(20,10))\n",
    "    sns.distplot(train_residuals, ax=ax[0,0], color=\"red\")\n",
    "    sns.distplot(eval_residuals, ax=ax[0,1], color=\"green\")\n",
    "    ax[0,0].set_title(\"Train residuals\")\n",
    "    ax[0,1].set_title(\"Eval residuals\")\n",
    "    ax[0,0].set_xlabel(\"$y_{true} - y_{pred}$\")\n",
    "    ax[0,1].set_xlabel(\"$y_{true} - y_{pred}$\")\n",
    "    ax[0,0].set_ylabel(\"density\")\n",
    "    ax[0,1].set_ylabel(\"density\")\n",
    "    \n",
    "    plot_acf(train_residuals, ax=ax[1,0])\n",
    "    plot_acf(eval_residuals, ax=ax[1,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "sampled_residuals = np.random.choice(train_residuals[:, 0], size=len(y_train), replace=True)\n",
    "sampled_residuals = sampled_residuals.reshape(-1,1)\n",
    "new_response = predictions_dict[\"train\"] + sampled_residuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "fig, ax = plt.subplots(2,2,figsize=(20,10))\n",
    "ax[0,0].plot(predictions_dict[\"train\"][0:200], 'o-', color=\"purple\")\n",
    "ax[0,0].set_title(\"Original fitted values $y_{pred}$ in \")\n",
    "ax[0,0].set_xlabel(\"200 example days\")\n",
    "ax[0,0].set_ylim(-0.4, 0.4)\n",
    "ax[0,0].set_ylabel(\"$y_{fitted}$\")\n",
    "\n",
    "ax[0,1].plot(new_response[0:200,0], 'o-', color=\"orange\")\n",
    "ax[0,1].set_title(\"Response values $y^{*}$ using sampled residuals\");\n",
    "ax[0,1].set_xlabel(\"200 example days\")\n",
    "ax[0,1].set_ylabel(\"$y^{*}$\");\n",
    "ax[0,1].set_ylim(-0.4, 0.4)\n",
    "\n",
    "ax[1,0].plot(sampled_residuals[0:200], 'o-', color=\"cornflowerblue\")\n",
    "ax[1,0].set_title(\"Sampled residuals\")\n",
    "ax[1,0].set_xlabel(\"200 example days\")\n",
    "ax[1,0].set_ylabel(\"$\\epsilon$\")\n",
    "\n",
    "ax[1,1].plot(y_train[0:200], 'o-', color=\"firebrick\")\n",
    "ax[1,1].set_title(\"True values $y_{train}$\")\n",
    "ax[1,1].set_xlabel(\"200 example days\")\n",
    "ax[1,1].set_ylabel(\"$y_{train}$\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "responses = []\n",
    "for n in range(100):\n",
    "    # sample residuals using the historical residuals found in train\n",
    "    sampled_residuals = np.random.choice(train_residuals[:, 0], size=len(y_eval), replace=True)\n",
    "    sampled_residuals = sampled_residuals.reshape(-1,1)\n",
    "    # create a synthetic future timeseries of eval by adding sampled residuals\n",
    "    new_response = predictions_dict[\"eval\"] + sampled_residuals\n",
    "    # reverse the scaling\n",
    "    new_response = scaler.inverse_transform(new_response)\n",
    "    # concat the first value of the evaluation series and the response series\n",
    "    new_response = np.hstack((timeseries[train_size], new_response[:,0]))\n",
    "    # reverse the differnciation (trend removal) using cumsum\n",
    "    new_response = np.cumsum(new_response)\n",
    "    # save the future timeseries\n",
    "    responses.append(new_response)\n",
    "    \n",
    "responses = np.array(responses)\n",
    "responses.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "y_eval.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "median_series = np.median(responses, axis=0)\n",
    "eval_series = scaler.inverse_transform(y_eval)\n",
    "eval_series = np.cumsum(np.hstack((timeseries[train_size-1], eval_series[:,0])))\n",
    "low_q = 0.25\n",
    "up_q = 0.75"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "plt.figure(figsize=(20,5))\n",
    "plt.plot(np.arange(0, len(median_series)), median_series, 'o-', label=\"median predicted series\")\n",
    "plt.plot(eval_series, '.-', color=\"cornflowerblue\", label=\"true eval series\")\n",
    "lower = np.quantile(responses, low_q, axis=0)\n",
    "upper = np.quantile(responses, up_q, axis=0)\n",
    "plt.fill_between(np.arange(0, len(median_series)), lower, upper, alpha=0.5)\n",
    "plt.title(\"Prediction interval {}% of eval timeseries\".format((up_q-low_q)*100));\n",
    "plt.xlabel(\"Days in eval\")\n",
    "plt.ylabel(\"Unit sales\");\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"six\"></a>\n",
    "## 6. Model Performance\n",
    "<a class=\"anchor\" id=\"1.1\"></a>\n",
    "<a href=#cont>Back to Table of Contents</a>\n",
    "\n",
    "---\n",
    "    \n",
    "| ⚡ Description: Model performance ⚡ |\n",
    "| :--------------------------- |\n",
    "| In this section, we compared the relative performance of the 3 ML models on a holdout dataset and made comments on what model is the best and why. |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tabletext\n",
    "\n",
    "s = [['No.','Model Name','RMSE'],\n",
    "        ['1','Facebook Prophet',3.2802],\n",
    "        ['2','LightGBM Regressor',1.8299],\n",
    "        ['3','LSTM',1.8667]\n",
    "      ]\n",
    "print(tabletext.to_text(s))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"seven\"></a>\n",
    "## 7. Model Explanations\n",
    "<a class=\"anchor\" id=\"1.1\"></a>\n",
    "<a href=#cont>Back to Table of Contents</a>\n",
    "\n",
    "---\n",
    "    \n",
    "| ⚡ Description: Model explanation ⚡ |\n",
    "| :--------------------------- |\n",
    "| In this section, we discussed how the best performing model works in a simple way so that both technical and non-technical stakeholders can grasp the intuition behind the model's inner workings. |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LGBM - Light Gradient Boosting Machine\n",
    "As seen from the above models, the `LGBMRegressor model`, from the `LGBM` module performs best among other models.\n",
    "\n",
    "It is a gradient boosting framework that makes use of tree based learning algorithms that is considered to be a very powerful algorithm when it comes to computation. It is considered to be a fast processing algorithm.\n",
    "\n",
    "\n",
    "While other algorithms trees grow horizontally, LightGBM algorithm grows vertically meaning it grows leaf-wise and other algorithms grow level-wise. LightGBM chooses the leaf with large loss to grow.\n",
    "\n",
    "It uses two novel techniques: **Gradient-based One Side Sampling (GOSS)** and **Exclusive Feature Bundling (EFB)** which fulfills the limitations of histogram-based algorithm that is primarily used in all GBDT (Gradient Boosting Decision Tree) frameworks. The two techniques of **GOSS** and **EFB** comprise together to make the model work efficiently and provide it a cutting edge over other GBDT frameworks <a href=#ref2>((LightGBM (Light Gradient Boosting Machine), 2021)</a>.\n",
    "\n",
    "\n",
    "\n",
    "The LightGBM model is considered to be a really fast algorithm and (arguably) the most used algorithm in machine learning when it comes to getting fast and high accuracy results <a href=#ref3>(What Is LightGBM Algorithm, How to Use It?, 2020)</a>. This is probably because of its `light` computation power and giving results faster. It also takes less memory to run and is able to deal with large amounts of data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"eight\"></a>\n",
    "## 8. References\n",
    "<a class=\"anchor\" id=\"1.1\"></a>\n",
    "<a href=#cont>Back to Table of Contents</a>\n",
    "\n",
    "---\n",
    "    \n",
    "| ⚡ Description: Reference ⚡ |\n",
    "| :--------------------------- |\n",
    "| In this section, we listed the references to the useful resources we discovered while working on this project. |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"ref2\"></a>\n",
    "- LightGBM (Light Gradient Boosting Machine). (2021, December 22). GeeksforGeeks. Retrieved October 28, 2022, from https://www.geeksforgeeks.org/lightgbm-light-gradient-boosting-machine/\n",
    "\n",
    "<a id=\"ref3\"></a>\n",
    "- What is LightGBM Algorithm, How to use it? (2020, June 25). Analytics Steps. Retrieved October 28, 2022, from https://www.analyticssteps.com/blogs/what-light-gbm-algorithm-how-use-it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"nine\"></a>\n",
    "## 9. Acknowledgement\n",
    "<a class=\"anchor\" id=\"1.1\"></a>\n",
    "<a href=#cont>Back to Table of Contents</a>\n",
    "\n",
    "---\n",
    "    \n",
    "| ⚡ Description: Reference ⚡ |\n",
    "| :--------------------------- |\n",
    "| In this section, we acknowledged the contributions of people who made this project successful. |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import date\n",
    "today = date.today()\n",
    "project_name = \"Developing an Opinionated Sales Forecasting Model\"\n",
    "contributors = {\n",
    "                'Kwazi Mbetu':\"Supervisor\",\n",
    "                'John Mohale': \"Supervisor\",\n",
    "                'Paul dos Santos': \"Technical Consultant\",\n",
    "                'Muzi Xaba': \"System Admin\",\n",
    "                'Zintle Faltein': \"Head Teacher\",\n",
    "                'Andile Skosana': \"System Admin\"\n",
    "               }\n",
    "\n",
    "for names, role in zip(contributors.keys(), contributors.values()):\n",
    "    print(\"\"\"\n",
    "Dear {},\n",
    "            An Open Appreciation from EDSA Internship Group Team 28\n",
    "    We really acknowledge and appreciate your contributions towards the success of\n",
    "our project: {}.\n",
    "    Your role as a {} really helped us scale through the project. You were there\n",
    "for us when we needed you. Keep doing amazing things.\n",
    "\n",
    "Regards,\n",
    "Josh (for Team 28)\n",
    "{}\n",
    "    \"\"\".format(names,project_name, role, today))\n",
    "    print(\"-\"*90)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "57bc2b6ce032b5f0e93daa91901b7ea38a856826ef43aa9e95b6d3999f5310df"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
